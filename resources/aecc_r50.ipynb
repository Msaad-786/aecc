{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \".\"))  # add parent dir to path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchinfo\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split, sampler\n",
    "from torchvision import datasets, models\n",
    "from torchvision import transforms as T  # for simplifying the transforms\n",
    "import timm\n",
    "\n",
    "# Custom imports\n",
    "from config import *\n",
    "from data import *\n",
    "\n",
    "# Use STIX font for math plotting\n",
    "plt.rcParams[\"font.family\"] = \"STIXGeneral\"\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from termcolor import colored\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cfg = get_imagenette_config()\n",
    "print(colored(f\"Config:\", \"green\"))\n",
    "print(cfg)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(colored(f\"Using device:\", \"green\"), device)\n",
    "\n",
    "# Seed for reproducability\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(cfg.seed)\n",
    "np.random.seed(np.array(cfg.seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_loader, _ = get_dataloader(\n",
    "    cfg.data_root,\n",
    "    \"train\",\n",
    "    cfg.batch_size,\n",
    "    cfg.num_workers,\n",
    "    noise_factor=0.5,\n",
    ")\n",
    "\n",
    "# Visualize 4 images; both original and noisy\n",
    "fig, ax = plt.subplots(2, 4, figsize=(8, 4))\n",
    "fig.suptitle(\"Randomly sampled images from the training set\")\n",
    "\n",
    "for i, (img, noisy_img) in enumerate(vis_loader):\n",
    "    if i == 4:\n",
    "        break\n",
    "    ax[0, i].imshow(img[0].permute(1, 2, 0), cmap=\"gray\")\n",
    "    ax[0, i].set_title(\"Original\")\n",
    "    ax[0, i].axis(\"off\")\n",
    "    ax[1, i].imshow(noisy_img[0].permute(1, 2, 0), cmap=\"gray\")\n",
    "    ax[1, i].set_title(\"Noisy\")\n",
    "    ax[1, i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The idea is to model the transmission of images [of shape 224x224x3] through a noisy \n",
    "(wireless channel using an autoencoder. The encoder is trained to compress the image \n",
    "into a latent vector, which is then transmitted through the channel. The decoder is \n",
    "trained to reconstruct the image from the latent vector. The encoder and decoder are \n",
    "trained jointly to minimize the reconstruction loss.\n",
    "\n",
    "The encoder uses a ResNet50 model, which is a convolutional neural network. The decoder \n",
    "uses transposed convolutions to upsample the latent vector into an image. The encoder \n",
    "and decoder are trained jointly to minimize the reconstruction loss.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "img_size = (224, 224)  # size of the image\n",
    "\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, num_channels=3, latent_dim=2048):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder (pretrained ResNet50)\n",
    "        self.encoder = nn.Sequential(\n",
    "            *list(torchvision.models.resnet50(pretrained=True).children())[:-2]\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 1024, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, num_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the denoising autoencoder\n",
    "x = torch.randn(1, num_channels, *img_size)\n",
    "x = x.to(device)\n",
    "\n",
    "dae = DenoisingAutoencoder().to(device)\n",
    "\n",
    "out = dae(x)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_dataloader(\n",
    "    cfg.data_root, \"train\", cfg.batch_size, cfg.num_workers, noise_factor=cfg.noise_factor\n",
    ")\n",
    "test_loader = get_dataloader(\n",
    "    cfg.data_root,\n",
    "    \"test\",\n",
    "    cfg.batch_size,\n",
    "    cfg.num_workers,\n",
    "    shuffle=False,\n",
    "    noise_factor=cfg.noise_factor,\n",
    ")\n",
    "\n",
    "print(colored(f\"Train loader batches:\", \"green\"), len(train_loader))\n",
    "print(colored(f\"Test loader batches:\", \"green\"), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dae.parameters(), lr=cfg.lr)\n",
    "learning_rate_scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=cfg.lr_step_size, gamma=cfg.lr_gamma\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    # Train\n",
    "    dae.train()\n",
    "    train_epoch_loss = 0.0\n",
    "    with tqdm(\n",
    "        train_loader, desc=f\"Epoch {epoch+1}/{cfg.num_epochs}\", unit=\"batch\"\n",
    "    ) as train_bar:\n",
    "        for i, (imgs, noisy_imgs) in enumerate(train_bar):\n",
    "            # noisy_imgs = imgs + cfg.noise_factor * torch.randn(*imgs.shape)\n",
    "            # noisy_imgs = torch.clamp(noisy_imgs, 0.0, 1.0)\n",
    "            imgs = imgs.to(device)\n",
    "            noisy_imgs = noisy_imgs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = dae(noisy_imgs)\n",
    "            loss = criterion(out, imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_epoch_loss += loss.item()\n",
    "            train_bar.set_postfix(train_loss=train_epoch_loss / (i + 1))\n",
    "\n",
    "    train_loss.append(train_epoch_loss / len(train_loader))\n",
    "\n",
    "    # Validation\n",
    "    dae.eval()\n",
    "    val_epoch_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, noisy_imgs in val_loader:\n",
    "            # noisy_imgs = imgs + cfg.noise_factor * torch.randn(*imgs.shape)\n",
    "            # noisy_imgs = torch.clamp(noisy_imgs, 0.0, 1.0)\n",
    "            imgs = imgs.to(device)\n",
    "            noisy_imgs = noisy_imgs.to(device)\n",
    "\n",
    "            out = dae(noisy_imgs)\n",
    "            loss = criterion(out, imgs)\n",
    "            val_epoch_loss += loss.item()\n",
    "\n",
    "    val_loss.append(val_epoch_loss / len(val_loader))\n",
    "\n",
    "    learning_rate_scheduler.step()\n",
    "\n",
    "    # Log results\n",
    "    print(\n",
    "        colored(\"Epoch\", \"green\")\n",
    "        + f\" {epoch+1}/{cfg.num_epochs} | \"\n",
    "        + colored(\"Train loss:\", \"yellow\")\n",
    "        + f\" {train_epoch_loss/len(train_loader):.5f} | \"\n",
    "        + colored(\"Validation loss:\", \"red\")\n",
    "        + f\" {val_epoch_loss/len(val_loader):.5f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training and validation loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_loss, label=\"Train loss\")\n",
    "plt.plot(val_loss, label=\"Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(dae.state_dict(), os.path.join(cfg.model_dir, \"dae_imagenette.pth\"))\n",
    "print(colored(f\"Model saved to {cfg.model_dir}\", \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae = DenoisingAutoencoder()\n",
    "dae.load_state_dict(torch.load(os.path.join(cfg.model_dir, \"dae_imagenette.pth\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the output of the autoencoder\n",
    "imgs, _ = next(iter(test_loader))\n",
    "noisy_imgs = imgs + cfg.noise_factor * torch.randn(*imgs.shape)\n",
    "out = dae(noisy_imgs).detach().cpu()\n",
    "\n",
    "fig, ax = plt.subplots(3, 4, figsize=(8, 6))\n",
    "\n",
    "for i in range(4):\n",
    "    ax[0, i].imshow(torch.clamp(imgs[i], 0, 1).permute(1, 2, 0))\n",
    "    ax[0, i].set_title(\"Original\")\n",
    "    ax[0, i].axis(\"off\")\n",
    "    ax[1, i].imshow(torch.clamp(noisy_imgs[i], 0, 1).permute(1, 2, 0))\n",
    "    ax[1, i].set_title(\"Noisy\")\n",
    "    ax[1, i].axis(\"off\")\n",
    "    ax[2, i].imshow(torch.clamp(out[i], 0, 1).permute(1, 2, 0))\n",
    "    ax[2, i].set_title(\"Reconstructed\")\n",
    "    ax[2, i].axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
