{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \".\"))  # add parent dir to path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchinfo\n",
    "\n",
    "# Custom imports\n",
    "from config import *\n",
    "from data import *\n",
    "\n",
    "# Use STIX font for math plotting\n",
    "plt.rcParams[\"font.family\"] = \"STIXGeneral\"\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from termcolor import colored\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cfg = get_mnist_config()\n",
    "print(colored(f\"Config:\", \"green\"))\n",
    "print(cfg)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(colored(f\"Using device:\", \"green\"), device)\n",
    "\n",
    "# Seed for reproducability\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(cfg.seed)\n",
    "np.random.seed(np.array(cfg.seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNISt dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=cfg.data_root,\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=cfg.data_root,\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "# Split the training dataset into training and validation datasets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.num_workers,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.num_workers,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.num_workers,\n",
    ")\n",
    "\n",
    "print(colored(f\"Train loader batches:\", \"green\"), len(train_loader))\n",
    "print(colored(f\"Test loader batches:\", \"green\"), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 4 images; both original and noisy\n",
    "fig, ax = plt.subplots(2, 4, figsize=(8, 4))\n",
    "fig.suptitle(\"Randomly sampled images from the training set\")\n",
    "\n",
    "for i, (img, _) in enumerate(train_loader):\n",
    "    if i == 4:\n",
    "        break\n",
    "    noisy_img = img + torch.randn_like(img) * cfg.noise_factor\n",
    "    ax[0, i].imshow(img[0].permute(1, 2, 0), cmap=\"gray\")\n",
    "    ax[0, i].set_title(\"Original\")\n",
    "    ax[0, i].axis(\"off\")\n",
    "    ax[1, i].imshow(noisy_img[0].permute(1, 2, 0), cmap=\"gray\")\n",
    "    ax[1, i].set_title(\"Noisy\")\n",
    "    ax[1, i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image to Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(batch, patch_size):\n",
    "    \"\"\"\n",
    "    Extract patches from a batch of images.\n",
    "\n",
    "    Args:\n",
    "        batch: batch of images\n",
    "        patch_size: size of the patch to be extracted\n",
    "    \"\"\"\n",
    "    b, c, h, w = batch.shape\n",
    "    assert (\n",
    "        h % patch_size[0] == 0 and w % patch_size[1] == 0\n",
    "    ), \"Patch size should be a factor of image height and width\"\n",
    "\n",
    "    ph, pw = patch_size  # patch height and width\n",
    "    nh, nw = h // ph, w // pw  # number of patches along height and width\n",
    "\n",
    "    batch_patches = torch.reshape(batch, (b, c, nh, ph, nw, pw))\n",
    "    batch_patches = torch.permute(batch_patches, (0, 1, 2, 4, 3, 5))\n",
    "\n",
    "    return batch_patches\n",
    "\n",
    "\n",
    "imgs, _ = next(iter(train_loader))\n",
    "img_patches = extract_patches(imgs, (4, 4))\n",
    "\n",
    "# Visualize one image and its patches\n",
    "patches = img_patches[0]\n",
    "c, nh, nw, ph, pw = patches.shape\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(imgs[0].permute(1, 2, 0), cmap=\"gray\")\n",
    "plt.title(\"Original image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i in range(nh):\n",
    "    for j in range(nw):\n",
    "        plt.subplot(nh, nw, i * nw + j + 1)\n",
    "        plt.imshow(patches[:, i, j].permute(1, 2, 0), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.suptitle(\"Patches extracted from the image\", y=0.935)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The idea is to model the transmission of images [of shape 224x224x3] through a\n",
    "noisy (wireless) channel using an autoencoder. The encoder is trained to compress \n",
    "the image into a latent vector, which is then transmitted through the channel. \n",
    "The decoder is trained to reconstruct the image from the latent vector. The \n",
    "encoder and decoder are trained jointly to minimize the reconstruction loss.\n",
    "\n",
    "The encoder used a vision transformer (ViT) encoder, which takes in patches of \n",
    "the image as input. The decoder used transposed convolutions to upsample the \n",
    "latent vector into an image. The encoder and decoder are trained jointly to \n",
    "minimize the reconstruction loss.\n",
    "\"\"\"\n",
    "\n",
    "# Parameters\n",
    "img_size = (28, 28)  # size of the image\n",
    "patch_size = (4, 4)  # size of the patch\n",
    "num_channels = 1  # number of channels in the image\n",
    "proj_dim = 64  # dimension of the projection head\n",
    "num_heads = 4  # number of heads in the transformer encoder\n",
    "dim_feedforward = 128  # dimension of the feedforward network in transformer encoder\n",
    "blocks = 2  # number of transformer encoder blocks\n",
    "mlp_units = [2048, 1024]  # units in the MLP of the projection head\n",
    "latent_dim = 16  # dimension of the latent vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Img2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to sequence model.\n",
    "\n",
    "    Args:\n",
    "        img_size: size of the image\n",
    "        patch_size: size of the patch to be extracted\n",
    "        num_channels: number of channels in the image\n",
    "        proj_dim: projection dimension\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size, patch_size, num_channels, proj_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "\n",
    "        nh, nw = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n",
    "        n_tokens = nh * nw\n",
    "\n",
    "        token_dim = patch_size[0] * patch_size[1] * num_channels\n",
    "        self.linear = nn.Linear(token_dim, proj_dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, proj_dim))\n",
    "        self.pos_emb = nn.Parameter(torch.randn(n_tokens, proj_dim))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = extract_patches(batch, self.patch_size)\n",
    "        b, c, nh, nw, ph, pw = batch.shape\n",
    "\n",
    "        # Flatten the patches\n",
    "        batch = torch.permute(batch, (0, 2, 3, 4, 5, 1))\n",
    "        batch = torch.reshape(batch, (b, nh * nw, ph * pw * c))\n",
    "\n",
    "        batch = self.linear(batch)\n",
    "        cls_token = self.cls_token.expand(b, -1, -1)\n",
    "        emb = batch + self.pos_emb\n",
    "        batch = torch.cat((cls_token, emb), dim=1)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_units, out_features):\n",
    "        super().__init__()\n",
    "        self.layers = self._build_layers(in_features, hidden_units, out_features)\n",
    "\n",
    "    def _build_layers(self, in_features, hidden_units, out_features):\n",
    "        dims = [in_features] + hidden_units + [out_features]\n",
    "        layers = []\n",
    "\n",
    "        for dim1, dim2 in zip(dims[:-2], dims[1:-1]):\n",
    "            layers.append(nn.Linear(dim1, dim2))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        layers.append(nn.Linear(dims[-2], dims[-1]))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size,\n",
    "        patch_size,\n",
    "        num_channels,\n",
    "        proj_dim,\n",
    "        num_heads,\n",
    "        dim_feedforward,\n",
    "        blocks,\n",
    "        mlp_units,\n",
    "        latent_dim,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img2seq = Img2Seq(img_size, patch_size, num_channels, proj_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            proj_dim, num_heads, dim_feedforward, activation=\"gelu\", batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, blocks)\n",
    "\n",
    "        # Add a fully connected layer for classification\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            3200,  # adjust this to match the output of the transformer encoder\n",
    "            mlp_units,\n",
    "            latent_dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = self.img2seq(batch)\n",
    "        batch = self.transformer_encoder(batch)\n",
    "\n",
    "        # Flatten the output of the transformer encoder\n",
    "        batch = self.flatten(batch)\n",
    "        batch = F.layer_norm(batch, batch.shape[1:])\n",
    "        batch = self.dropout(batch)\n",
    "\n",
    "        # Pass the flattened output to the classifier\n",
    "        batch = self.mlp(batch)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "# Test the ViT encoder\n",
    "x = torch.randn(cfg.batch_size, num_channels, *img_size)\n",
    "x = x.to(device)\n",
    "vit = ViT(\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    num_channels,\n",
    "    proj_dim,\n",
    "    num_heads,\n",
    "    dim_feedforward,\n",
    "    blocks,\n",
    "    mlp_units,\n",
    "    latent_dim,\n",
    ")\n",
    "\n",
    "vit.to(device)\n",
    "out = vit(x)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(vit, input_size=(cfg.batch_size, num_channels, *img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 8 * 7 * 7)\n",
    "        self.conv1 = nn.ConvTranspose2d(\n",
    "            8, 8, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.conv2 = nn.ConvTranspose2d(\n",
    "            8, 16, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(16, num_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = x.view(-1, 8, 7, 7)  # reshape operation\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.sigmoid(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size,\n",
    "        patch_size,\n",
    "        num_channels,\n",
    "        proj_dim,\n",
    "        num_heads,\n",
    "        dim_feedforward,\n",
    "        blocks,\n",
    "        mlp_units,\n",
    "        latent_dim,\n",
    "    ):\n",
    "        super().__init__()  # Call parent's __init__ method first\n",
    "\n",
    "        self.encoder = ViT(\n",
    "            img_size,\n",
    "            patch_size,\n",
    "            num_channels,\n",
    "            proj_dim,\n",
    "            num_heads,\n",
    "            dim_feedforward,\n",
    "            blocks,\n",
    "            mlp_units,\n",
    "            latent_dim,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(latent_dim, num_channels)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = self.encoder(batch)\n",
    "        batch = self.decoder(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "# Test the denoising autoencoder\n",
    "x = torch.randn(cfg.batch_size, num_channels, *img_size)\n",
    "x = x.to(device)\n",
    "\n",
    "dae = DenoisingAutoencoder(\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    num_channels,\n",
    "    proj_dim,\n",
    "    num_heads,\n",
    "    dim_feedforward,\n",
    "    blocks,\n",
    "    mlp_units,\n",
    "    latent_dim,\n",
    ")\n",
    "\n",
    "dae.to(device)\n",
    "out = dae(x)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dae.parameters(), lr=cfg.lr)\n",
    "learning_rate_scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=cfg.lr_step_size, gamma=cfg.lr_gamma\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    # Train\n",
    "    dae.train()\n",
    "    train_epoch_loss = 0.0\n",
    "    with tqdm(\n",
    "        train_loader, desc=f\"Epoch {epoch+1}/{cfg.num_epochs}\", unit=\"batch\"\n",
    "    ) as train_bar:\n",
    "        for i, (imgs, _) in enumerate(train_bar):\n",
    "            noisy_imgs = imgs + cfg.noise_factor * torch.randn(*imgs.shape)\n",
    "            noisy_imgs = torch.clamp(noisy_imgs, 0.0, 1.0)\n",
    "            imgs = imgs.to(device)\n",
    "            noisy_imgs = noisy_imgs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = dae(noisy_imgs)\n",
    "            loss = criterion(out, imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_epoch_loss += loss.item()\n",
    "            train_bar.set_postfix(train_loss=train_epoch_loss / (i + 1))\n",
    "\n",
    "    train_loss.append(train_epoch_loss / len(train_loader))\n",
    "\n",
    "    # Validation\n",
    "    dae.eval()\n",
    "    val_epoch_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in val_loader:\n",
    "            noisy_imgs = imgs + cfg.noise_factor * torch.randn(*imgs.shape)\n",
    "            noisy_imgs = torch.clamp(noisy_imgs, 0.0, 1.0)\n",
    "            imgs = imgs.to(device)\n",
    "            noisy_imgs = noisy_imgs.to(device)\n",
    "\n",
    "            out = dae(noisy_imgs)\n",
    "            loss = criterion(out, imgs)\n",
    "            val_epoch_loss += loss.item()\n",
    "\n",
    "    val_loss.append(val_epoch_loss / len(val_loader))\n",
    "\n",
    "    learning_rate_scheduler.step()\n",
    "\n",
    "    # Log results\n",
    "    print(\n",
    "        colored(\"Epoch\", \"green\")\n",
    "        + f\" {epoch+1}/{cfg.num_epochs} | \"\n",
    "        + colored(\"Train loss:\", \"yellow\")\n",
    "        + f\" {train_epoch_loss/len(train_loader):.5f} | \"\n",
    "        + colored(\"Validation loss:\", \"red\")\n",
    "        + f\" {val_epoch_loss/len(val_loader):.5f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training and validation loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_loss, label=\"Train loss\")\n",
    "plt.plot(val_loss, label=\"Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(dae.state_dict(), os.path.join(cfg.model_dir, \"dae_mnist.pth\"))\n",
    "print(colored(f\"Model saved to {cfg.model_dir}\", \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae = DenoisingAutoencoder(\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    num_channels,\n",
    "    proj_dim,\n",
    "    num_heads,\n",
    "    dim_feedforward,\n",
    "    blocks,\n",
    "    mlp_units,\n",
    "    latent_dim,\n",
    ")\n",
    "dae.load_state_dict(torch.load(os.path.join(cfg.model_dir, \"dae_mnist.pth\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the output of the autoencoder\n",
    "imgs, _ = next(iter(test_loader))\n",
    "noisy_imgs = imgs + cfg.noise_factor * torch.randn(*imgs.shape)\n",
    "out = dae(noisy_imgs).detach().cpu()\n",
    "\n",
    "fig, ax = plt.subplots(3, 4, figsize=(8, 6))\n",
    "\n",
    "for i in range(4):\n",
    "    ax[0, i].imshow(imgs[i][0], cmap=\"gray\")\n",
    "    ax[0, i].set_title(\"Original\")\n",
    "    ax[0, i].axis(\"off\")\n",
    "    ax[1, i].imshow(noisy_imgs[i][0], cmap=\"gray\")\n",
    "    ax[1, i].set_title(\"Noisy\")\n",
    "    ax[1, i].axis(\"off\")\n",
    "    ax[2, i].imshow(out[i][0], cmap=\"gray\")\n",
    "    ax[2, i].set_title(\"Reconstructed\")\n",
    "    ax[2, i].axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
